{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T20:18:39.062963Z",
     "iopub.status.busy": "2021-12-11T20:18:39.062424Z",
     "iopub.status.idle": "2021-12-11T20:18:45.455601Z",
     "shell.execute_reply": "2021-12-11T20:18:45.454773Z",
     "shell.execute_reply.started": "2021-12-11T20:18:39.062865Z"
    },
    "id": "DEfSbAA4QHas"
   },
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "GPU device not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a532c2b9b173>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgpu_device_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'/device:GPU:0'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GPU device not found'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Found GPU at: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemError\u001b[0m: GPU device not found"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T20:18:45.874334Z",
     "iopub.status.busy": "2021-12-11T20:18:45.873847Z",
     "iopub.status.idle": "2021-12-11T20:18:55.791986Z",
     "shell.execute_reply": "2021-12-11T20:18:55.791039Z",
     "shell.execute_reply.started": "2021-12-11T20:18:45.874294Z"
    },
    "id": "0NmMdkZO8R6q"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch-pretrained-bert pytorch-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T20:18:57.162918Z",
     "iopub.status.busy": "2021-12-11T20:18:57.162641Z",
     "iopub.status.idle": "2021-12-11T20:18:59.523807Z",
     "shell.execute_reply": "2021-12-11T20:18:59.523095Z",
     "shell.execute_reply.started": "2021-12-11T20:18:57.162886Z"
    },
    "id": "Ok002ceNB8E7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification, BertConfig\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "import gc\n",
    "# % matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T20:19:00.011382Z",
     "iopub.status.busy": "2021-12-11T20:19:00.011099Z",
     "iopub.status.idle": "2021-12-11T20:19:02.692216Z",
     "shell.execute_reply": "2021-12-11T20:19:02.691193Z",
     "shell.execute_reply.started": "2021-12-11T20:19:00.011354Z"
    },
    "id": "oYsV4H8fCpZ-"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "guw6ZNtaswKc"
   },
   "source": [
    "## Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T20:19:03.913856Z",
     "iopub.status.busy": "2021-12-11T20:19:03.913134Z",
     "iopub.status.idle": "2021-12-11T20:19:04.210893Z",
     "shell.execute_reply": "2021-12-11T20:19:04.210198Z",
     "shell.execute_reply.started": "2021-12-11T20:19:03.913812Z"
    },
    "id": "_UkeC7SG2krJ"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../input/cqa-dataset/cqa_queries.csv\", delimiter=',')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T20:19:06.331325Z",
     "iopub.status.busy": "2021-12-11T20:19:06.331011Z",
     "iopub.status.idle": "2021-12-11T20:19:06.352764Z",
     "shell.execute_reply": "2021-12-11T20:19:06.352117Z",
     "shell.execute_reply.started": "2021-12-11T20:19:06.331291Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T20:19:07.391895Z",
     "iopub.status.busy": "2021-12-11T20:19:07.391196Z",
     "iopub.status.idle": "2021-12-11T20:19:07.42167Z",
     "shell.execute_reply": "2021-12-11T20:19:07.42094Z",
     "shell.execute_reply.started": "2021-12-11T20:19:07.391854Z"
    }
   },
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(df['question'], df['main_category'], random_state=2018, test_size=0.1)\n",
    "print(train_X.shape)\n",
    "print(test_X.shape)\n",
    "print(train_y.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T20:19:09.69353Z",
     "iopub.status.busy": "2021-12-11T20:19:09.693148Z",
     "iopub.status.idle": "2021-12-11T20:19:09.891661Z",
     "shell.execute_reply": "2021-12-11T20:19:09.890714Z",
     "shell.execute_reply.started": "2021-12-11T20:19:09.693485Z"
    }
   },
   "outputs": [],
   "source": [
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T20:19:11.931346Z",
     "iopub.status.busy": "2021-12-11T20:19:11.931075Z",
     "iopub.status.idle": "2021-12-11T20:19:11.972001Z",
     "shell.execute_reply": "2021-12-11T20:19:11.97131Z",
     "shell.execute_reply.started": "2021-12-11T20:19:11.931317Z"
    },
    "id": "GuE5BqICAne2"
   },
   "outputs": [],
   "source": [
    "# Create sentence and label lists\n",
    "# sentences = df[\"question\"].values\n",
    "sentences = train_X.values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "# labels = df['main_category'].values\n",
    "labels = train_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T20:19:13.723834Z",
     "iopub.status.busy": "2021-12-11T20:19:13.723097Z",
     "iopub.status.idle": "2021-12-11T20:19:13.741524Z",
     "shell.execute_reply": "2021-12-11T20:19:13.740538Z",
     "shell.execute_reply.started": "2021-12-11T20:19:13.723785Z"
    }
   },
   "outputs": [],
   "source": [
    "label_list = train_y.unique()\n",
    "print(len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T20:19:14.714963Z",
     "iopub.status.busy": "2021-12-11T20:19:14.714712Z",
     "iopub.status.idle": "2021-12-11T20:19:14.900533Z",
     "shell.execute_reply": "2021-12-11T20:19:14.89942Z",
     "shell.execute_reply.started": "2021-12-11T20:19:14.714935Z"
    }
   },
   "outputs": [],
   "source": [
    "del train_X\n",
    "del train_y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ex5O1eV-Pfct"
   },
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T20:19:17.173227Z",
     "iopub.status.busy": "2021-12-11T20:19:17.172954Z",
     "iopub.status.idle": "2021-12-11T20:19:44.187199Z",
     "shell.execute_reply": "2021-12-11T20:19:44.185638Z",
     "shell.execute_reply.started": "2021-12-11T20:19:17.173197Z"
    },
    "id": "Z474sSC6oe7A"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T20:20:02.442076Z",
     "iopub.status.busy": "2021-12-11T20:20:02.441788Z",
     "iopub.status.idle": "2021-12-11T20:20:02.659944Z",
     "shell.execute_reply": "2021-12-11T20:20:02.659086Z",
     "shell.execute_reply.started": "2021-12-11T20:20:02.442045Z"
    }
   },
   "outputs": [],
   "source": [
    "del sentences\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T20:20:03.932096Z",
     "iopub.status.busy": "2021-12-11T20:20:03.931508Z",
     "iopub.status.idle": "2021-12-11T20:20:03.936041Z",
     "shell.execute_reply": "2021-12-11T20:20:03.935202Z",
     "shell.execute_reply.started": "2021-12-11T20:20:03.932057Z"
    },
    "id": "Cp9BPRd1tMIo"
   },
   "outputs": [],
   "source": [
    "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
    "# In the original paper, the authors used a length of 512.\n",
    "MAX_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T20:20:05.133631Z",
     "iopub.status.busy": "2021-12-11T20:20:05.133317Z",
     "iopub.status.idle": "2021-12-11T20:20:06.229705Z",
     "shell.execute_reply": "2021-12-11T20:20:06.22897Z",
     "shell.execute_reply.started": "2021-12-11T20:20:05.133599Z"
    },
    "id": "kDs-MYtYH8sL"
   },
   "outputs": [],
   "source": [
    "# Pad our input tokens\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts], maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T20:20:07.71198Z",
     "iopub.status.busy": "2021-12-11T20:20:07.71172Z",
     "iopub.status.idle": "2021-12-11T20:20:07.933896Z",
     "shell.execute_reply": "2021-12-11T20:20:07.933136Z",
     "shell.execute_reply.started": "2021-12-11T20:20:07.711951Z"
    }
   },
   "outputs": [],
   "source": [
    "del tokenized_texts\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T20:20:09.963879Z",
     "iopub.status.busy": "2021-12-11T20:20:09.963261Z",
     "iopub.status.idle": "2021-12-11T20:20:22.317561Z",
     "shell.execute_reply": "2021-12-11T20:20:22.316792Z",
     "shell.execute_reply.started": "2021-12-11T20:20:09.963839Z"
    },
    "id": "cDoC24LeEv3N"
   },
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T20:20:23.624963Z",
     "iopub.status.busy": "2021-12-11T20:20:23.624081Z",
     "iopub.status.idle": "2021-12-11T20:20:23.750958Z",
     "shell.execute_reply": "2021-12-11T20:20:23.75Z",
     "shell.execute_reply.started": "2021-12-11T20:20:23.624922Z"
    },
    "id": "aFbE-UHvsb7-"
   },
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2018, test_size=0.111)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T20:20:36.063748Z",
     "iopub.status.busy": "2021-12-11T20:20:36.063134Z",
     "iopub.status.idle": "2021-12-11T20:20:36.087357Z",
     "shell.execute_reply": "2021-12-11T20:20:36.086476Z",
     "shell.execute_reply.started": "2021-12-11T20:20:36.063707Z"
    }
   },
   "outputs": [],
   "source": [
    "del input_ids\n",
    "del labels\n",
    "del attention_masks\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T20:20:42.34229Z",
     "iopub.status.busy": "2021-12-11T20:20:42.341991Z",
     "iopub.status.idle": "2021-12-11T20:20:42.405474Z",
     "shell.execute_reply": "2021-12-11T20:20:42.404518Z",
     "shell.execute_reply.started": "2021-12-11T20:20:42.342231Z"
    }
   },
   "outputs": [],
   "source": [
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_labels = train_labels.astype('str')\n",
    "validation_labels = validation_labels.astype('str')\n",
    "\n",
    "train_labels = encoder.fit_transform(train_labels)\n",
    "validation_labels = encoder.fit_transform(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T20:20:44.391726Z",
     "iopub.status.busy": "2021-12-11T20:20:44.39119Z",
     "iopub.status.idle": "2021-12-11T20:20:45.520825Z",
     "shell.execute_reply": "2021-12-11T20:20:45.519889Z",
     "shell.execute_reply.started": "2021-12-11T20:20:44.391692Z"
    },
    "id": "jw5K2A5Ko1RF"
   },
   "outputs": [],
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T20:20:46.421921Z",
     "iopub.status.busy": "2021-12-11T20:20:46.42119Z",
     "iopub.status.idle": "2021-12-11T20:20:46.429025Z",
     "shell.execute_reply": "2021-12-11T20:20:46.428218Z",
     "shell.execute_reply.started": "2021-12-11T20:20:46.42188Z"
    },
    "id": "GEgLpFVlo1Z-"
   },
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNl8khAhPYju"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T20:20:48.323588Z",
     "iopub.status.busy": "2021-12-11T20:20:48.323032Z",
     "iopub.status.idle": "2021-12-11T20:21:08.009497Z",
     "shell.execute_reply": "2021-12-11T20:21:08.008718Z",
     "shell.execute_reply.started": "2021-12-11T20:20:48.323547Z"
    },
    "id": "gFsCTp_mporB"
   },
   "outputs": [],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
    "# config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "# config.num_labels = len(label_list)\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_list))\n",
    "# model = BertForSequenceClassification(config)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8o-VEBobKwHk"
   },
   "source": [
    "For the purposes of fine-tuning, we are following hyperparameter ranges:\n",
    "- Batch size: 16, 32\n",
    "- Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
    "- Number of epochs: 2, 3, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T20:22:36.334938Z",
     "iopub.status.busy": "2021-12-11T20:22:36.334521Z",
     "iopub.status.idle": "2021-12-11T20:22:36.344762Z",
     "shell.execute_reply": "2021-12-11T20:22:36.343512Z",
     "shell.execute_reply.started": "2021-12-11T20:22:36.334895Z"
    },
    "id": "QxSMw0FrptiL"
   },
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T20:22:37.491796Z",
     "iopub.status.busy": "2021-12-11T20:22:37.491039Z",
     "iopub.status.idle": "2021-12-11T20:22:37.496602Z",
     "shell.execute_reply": "2021-12-11T20:22:37.495669Z",
     "shell.execute_reply.started": "2021-12-11T20:22:37.491758Z"
    },
    "id": "GLs72DuMODJO"
   },
   "outputs": [],
   "source": [
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=2e-5,\n",
    "                     warmup=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T20:22:39.383083Z",
     "iopub.status.busy": "2021-12-11T20:22:39.382838Z",
     "iopub.status.idle": "2021-12-11T20:22:39.387688Z",
     "shell.execute_reply": "2021-12-11T20:22:39.386787Z",
     "shell.execute_reply.started": "2021-12-11T20:22:39.383053Z"
    },
    "id": "9cQNvaZ9bnyy"
   },
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T20:22:40.707969Z",
     "iopub.status.busy": "2021-12-11T20:22:40.707322Z",
     "iopub.status.idle": "2021-12-11T21:08:27.828086Z",
     "shell.execute_reply": "2021-12-11T21:08:27.827078Z",
     "shell.execute_reply.started": "2021-12-11T20:22:40.707926Z"
    },
    "id": "6J-FYdx6nFE_"
   },
   "outputs": [],
   "source": [
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "valid_loss_set = []\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 2\n",
    "\n",
    "# creating the tokenizer directory to save the model\n",
    "if not os.path.exists('tokenizer/'):\n",
    "    os.mkdir('tokenizer/')\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "  \n",
    "  \n",
    "  # Training\n",
    "  \n",
    "  # Set our model to training mode (as opposed to evaluation mode)\n",
    "  model.train()\n",
    "  \n",
    "  # Tracking variables\n",
    "  tr_loss = 0\n",
    "  nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "  # Train the data for one epoch\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # Clear out the gradients (by default they accumulate)\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "    train_loss_set.append(loss.item())    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    # Update parameters and take a step using the computed gradient\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    # Update tracking variables\n",
    "    tr_loss += loss.item()\n",
    "    nb_tr_examples += b_input_ids.size(0)\n",
    "    nb_tr_steps += 1\n",
    "  \n",
    "  train_loss_set.append(tr_loss/nb_tr_steps)\n",
    "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    \n",
    "    \n",
    "  # Validation\n",
    "\n",
    "  # Put model in evaluation mode to evaluate loss on the validation set\n",
    "  model.eval()\n",
    "\n",
    "  # Tracking variables \n",
    "  eval_loss, eval_accuracy = 0, 0\n",
    "  nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "  # Evaluate data for one epoch\n",
    "  for batch in validation_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    \n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    \n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "\n",
    "\n",
    "# model.save_pretrained('tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T21:17:15.033651Z",
     "iopub.status.busy": "2021-12-11T21:17:15.032896Z",
     "iopub.status.idle": "2021-12-11T21:17:15.832382Z",
     "shell.execute_reply": "2021-12-11T21:17:15.83146Z",
     "shell.execute_reply.started": "2021-12-11T21:17:15.033611Z"
    }
   },
   "outputs": [],
   "source": [
    "output_model_file = \"./tokenizer/model_file.bin\"\n",
    "output_config_file = \"./tokenizer/config_file.bin\"\n",
    "output_vocab_file = \"./tokenizer/\"\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "torch.save(model_to_save.state_dict(), output_model_file)\n",
    "model_to_save.config.to_json_file(output_config_file)\n",
    "tokenizer.save_vocabulary(output_vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyRa-5CcHv_g"
   },
   "source": [
    "## Training Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-G03mmwH3aI"
   },
   "source": [
    "Let's take a look at our training loss over all batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68xreA9JAmG5"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(train_loss_set)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
