{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-12-06T10:53:28.071137Z",
     "iopub.status.busy": "2021-12-06T10:53:28.070614Z",
     "iopub.status.idle": "2021-12-06T10:53:34.958333Z",
     "shell.execute_reply": "2021-12-06T10:53:34.957604Z",
     "shell.execute_reply.started": "2021-12-06T10:53:28.071019Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KiranDon\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "C:\\Users\\KiranDon\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\KiranDon\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\KiranDon\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "C:\\Users\\KiranDon\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\KiranDon\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\KiranDon\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "C:\\Users\\KiranDon\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\KiranDon\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\KiranDon\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "C:\\Users\\KiranDon\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n",
      "C:\\Users\\KiranDon\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py:32: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from ._gradient_boosting import predict_stages\n",
      "C:\\Users\\KiranDon\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py:32: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from ._gradient_boosting import predict_stages\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "import gensim\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas, xgboost, numpy, textblob, string\n",
    "from scipy import sparse\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Raw Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T10:53:37.022309Z",
     "iopub.status.busy": "2021-12-06T10:53:37.021882Z",
     "iopub.status.idle": "2021-12-06T10:53:37.347862Z",
     "shell.execute_reply": "2021-12-06T10:53:37.347032Z",
     "shell.execute_reply.started": "2021-12-06T10:53:37.022277Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The CDC currently reports 99031 deaths. In gen...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>States reported 1121 deaths a small rise from ...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Politically Correct Woman (Almost) Uses Pandem...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>#IndiaFightsCorona: We have 1524 #COVID testin...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Populous states can generate large case counts...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              tweet label\n",
       "0   1  The CDC currently reports 99031 deaths. In gen...  real\n",
       "1   2  States reported 1121 deaths a small rise from ...  real\n",
       "2   3  Politically Correct Woman (Almost) Uses Pandem...  fake\n",
       "3   4  #IndiaFightsCorona: We have 1524 #COVID testin...  real\n",
       "4   5  Populous states can generate large case counts...  real"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data.tsv\", sep='\\t')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6416</td>\n",
       "      <td>6417</td>\n",
       "      <td>???Autopsies prove that COVID-19 is??� a blood...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6417</td>\n",
       "      <td>6418</td>\n",
       "      <td>_A post claims a COVID-19 vaccine has already ...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6418</td>\n",
       "      <td>6419</td>\n",
       "      <td>Aamir Khan Donate 250 Cr. In PM Relief Cares Fund</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6419</td>\n",
       "      <td>6420</td>\n",
       "      <td>It has been 93 days since the last case of COV...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6420</td>\n",
       "      <td>6421</td>\n",
       "      <td>The House Democratic Caucus holds a moment of ...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              tweet label\n",
       "6416  6417  ???Autopsies prove that COVID-19 is??� a blood...  fake\n",
       "6417  6418  _A post claims a COVID-19 vaccine has already ...  fake\n",
       "6418  6419  Aamir Khan Donate 250 Cr. In PM Relief Cares Fund  fake\n",
       "6419  6420  It has been 93 days since the last case of COV...  real\n",
       "6420  6421  The House Democratic Caucus holds a moment of ...  real"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_punctuation(text):\n",
    "#     punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
    "#     return punctuationfree\n",
    "\n",
    "# # Removing punctuations\n",
    "# data['tweet']= data['tweet'].apply(lambda x:remove_punctuation(x))\n",
    "\n",
    "\n",
    "# # Converting text into lower case\n",
    "# data['tweet']= data['tweet'].apply(lambda x: x.lower())\n",
    "\n",
    "\n",
    "# def remove_stopwords(text):\n",
    "#     text = text.split(\" \")\n",
    "#     output= \" \".join([i for i in text if i not in stopwords])\n",
    "#     return output\n",
    "\n",
    "# stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# # Removing stopwords\n",
    "# for i in range(len(data)):\n",
    "#     data[\"tweet\"][i] = remove_stopwords(data[\"tweet\"][i])\n",
    "    \n",
    "\n",
    "# # Removing urls.....\n",
    "# for i in range(len(data)):\n",
    "#     temp = data[\"tweet\"][i].split(\" \")\n",
    "#     if temp[-1].startswith(\"http\"):\n",
    "#         temp.pop()\n",
    "#     data[\"tweet\"][i] = \" \".join(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['tweet']\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.3,random_state=53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T10:53:43.151206Z",
     "iopub.status.busy": "2021-12-06T10:53:43.150606Z",
     "iopub.status.idle": "2021-12-06T10:53:43.207854Z",
     "shell.execute_reply": "2021-12-06T10:53:43.20689Z",
     "shell.execute_reply.started": "2021-12-06T10:53:43.15117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real    2333\n",
      "fake    2161\n",
      "Name: label, dtype: int64\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(train_y.value_counts())\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = train_y.astype('str')\n",
    "test_y = test_y.astype('str')\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "test_y = encoder.fit_transform(test_y)\n",
    "\n",
    "classes = list(encoder.classes_)\n",
    "print(len(classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T10:53:45.83929Z",
     "iopub.status.busy": "2021-12-06T10:53:45.838519Z",
     "iopub.status.idle": "2021-12-06T10:53:45.844787Z",
     "shell.execute_reply": "2021-12-06T10:53:45.843979Z",
     "shell.execute_reply.started": "2021-12-06T10:53:45.839246Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "\n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "\n",
    "    \n",
    "    acc = metrics.accuracy_score(predictions, test_y)\n",
    "    f1 = metrics.f1_score(predictions, test_y, average='weighted')\n",
    "    #print(classification_report(predictions, test_y, target_names = list(encoder.classes_)))\n",
    "    return acc, f1, classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T16:27:06.325165Z",
     "iopub.status.busy": "2021-12-05T16:27:06.324618Z",
     "iopub.status.idle": "2021-12-05T16:27:09.795726Z",
     "shell.execute_reply": "2021-12-05T16:27:09.795114Z",
     "shell.execute_reply.started": "2021-12-05T16:27:06.325129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 900 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((4494, 14833), (1927, 14833))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(pd.concat([train_x, test_x], axis=0))\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.fit_transform(train_x)\n",
    "xvalid_count =  count_vect.transform(test_x)\n",
    "xtrain_count.shape, xvalid_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T16:27:11.451571Z",
     "iopub.status.busy": "2021-12-05T16:27:11.450959Z",
     "iopub.status.idle": "2021-12-05T16:29:08.024398Z",
     "shell.execute_reply": "2021-12-05T16:29:08.023479Z",
     "shell.execute_reply.started": "2021-12-05T16:27:11.451488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9231966787752984\n",
      "f1 Score: 0.9231675321926582\n",
      "Wall time: 4.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Extereme Gradient Boosting on Count Vectors\n",
    "accuracy, f1_score, classifier = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "# print(\"Xgb, Count Vectors: \", accuracy, f1_score)\n",
    "print(\"Accuracy Score: {}\".format(accuracy))\n",
    "print(\"f1 Score: {}\".format(f1_score))\n",
    "\n",
    "# Save the model to output folder\n",
    "filename = 'count_vec.pkl'\n",
    "with open(filename, 'wb') as fp:\n",
    "    pickle.dump(classifier, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del xtrain_count\n",
    "del xvalid_count\n",
    "del count_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word level TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T16:29:23.412895Z",
     "iopub.status.busy": "2021-12-05T16:29:23.412626Z",
     "iopub.status.idle": "2021-12-05T16:29:26.884251Z",
     "shell.execute_reply": "2021-12-05T16:29:26.883267Z",
     "shell.execute_reply.started": "2021-12-05T16:29:23.412865Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 619 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern = r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(pd.concat([train_x, test_x], axis=0))\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T16:29:28.114912Z",
     "iopub.status.busy": "2021-12-05T16:29:28.11465Z",
     "iopub.status.idle": "2021-12-05T16:35:36.758652Z",
     "shell.execute_reply": "2021-12-05T16:35:36.75781Z",
     "shell.execute_reply.started": "2021-12-05T16:29:28.114886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.915412558380903\n",
      "f1 Score: 0.9154155332432391\n",
      "Wall time: 3.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy, f1_score, classifier = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
    "# print(\"Xgb, WordLevel TF-IDF: \", accuracy, f1_score)\n",
    "print(\"Accuracy Score: {}\".format(accuracy))\n",
    "print(\"f1 Score: {}\".format(f1_score))\n",
    "\n",
    "# Save the model to output folder\n",
    "filename = 'word-tfidf.pkl'\n",
    "with open(filename, 'wb') as fp:\n",
    "    pickle.dump(classifier, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del xtrain_tfidf\n",
    "del xvalid_tfidf\n",
    "del tfidf_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngram word Level TF-iDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T16:50:22.920639Z",
     "iopub.status.busy": "2021-12-05T16:50:22.920353Z",
     "iopub.status.idle": "2021-12-05T16:50:34.010999Z",
     "shell.execute_reply": "2021-12-05T16:50:34.0092Z",
     "shell.execute_reply.started": "2021-12-05T16:50:22.920608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', ngram_range=(2,3), max_features=25000)\n",
    "tfidf_vect_ngram.fit(pd.concat([train_x, test_x], axis=0))\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T16:50:37.816898Z",
     "iopub.status.busy": "2021-12-05T16:50:37.816638Z",
     "iopub.status.idle": "2021-12-05T16:53:55.539107Z",
     "shell.execute_reply": "2021-12-05T16:53:55.53828Z",
     "shell.execute_reply.started": "2021-12-05T16:50:37.81687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8401660612350804\n",
      "f1 Score: 0.8401011489825361\n",
      "Wall time: 2.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
    "accuracy, f1_score, classifier = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram.tocsc(), train_y, xvalid_tfidf_ngram.tocsc())\n",
    "# print(\"Xgb, Ngram WordLevel Vectors: \", accuracy, f1_score)\n",
    "print(\"Accuracy Score: {}\".format(accuracy))\n",
    "print(\"f1 Score: {}\".format(f1_score))\n",
    "\n",
    "# Save the model to output folder\n",
    "filename = 'ngram-tfidf.pkl'\n",
    "with open(filename, 'wb') as fp:\n",
    "    pickle.dump(classifier, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del xtrain_tfidf_ngram\n",
    "del xvalid_tfidf_ngram\n",
    "del tfidf_vect_ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char Level TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T10:53:53.202616Z",
     "iopub.status.busy": "2021-12-06T10:53:53.201738Z",
     "iopub.status.idle": "2021-12-06T10:54:14.104329Z",
     "shell.execute_reply": "2021-12-06T10:54:14.103561Z",
     "shell.execute_reply.started": "2021-12-06T10:53:53.202574Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', ngram_range=(2,3), max_features=25000)\n",
    "tfidf_vect_ngram_chars.fit(pd.concat([train_x, test_x], axis=0))\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(test_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T10:54:18.388934Z",
     "iopub.status.busy": "2021-12-06T10:54:18.388672Z",
     "iopub.status.idle": "2021-12-06T11:52:08.060179Z",
     "shell.execute_reply": "2021-12-06T11:52:08.059401Z",
     "shell.execute_reply.started": "2021-12-06T10:54:18.388908Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9538142189932538\n",
      "f1 Score: 0.9538064754702225\n",
      "Wall time: 42.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
    "accuracy, f1_score, classifier = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\n",
    "# print(\"Xgb, CharLevel Vectors: \", accuracy, f1_score)\n",
    "print(\"Accuracy Score: {}\".format(accuracy))\n",
    "print(\"f1 Score: {}\".format(f1_score))\n",
    "\n",
    "# Save the model to output folder\n",
    "filename = 'char-tfidf.pkl'\n",
    "with open(filename, 'wb') as fp:\n",
    "    pickle.dump(classifier, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real\n"
     ]
    }
   ],
   "source": [
    "# tweet = [\"China president xi jinping visited masjid and request Muslims for dua in present crisis country going through.we need your help.\"]\n",
    "tweet = [\"The CDC currently reports 99031 deaths. In general the discrepancies in death counts between different sources are small and explicable. The death toll stands at roughly 100000 people today.\"]\n",
    "\n",
    "chars =  tfidf_vect_ngram_chars.transform(tweet)\n",
    "result = classifier.predict(chars)[0]\n",
    "if result==0:\n",
    "    print(\"Fake\")\n",
    "else:\n",
    "    print(\"Real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
